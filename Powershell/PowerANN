class Layer {
    [int] $size = 0
    [int] $activation
    $x = @()
    $z = @()
    $bias = @()
    $output = @()
    $weights = @()
        
    Layer($size, $activation) {
        $this.size = $size
        $this.activation = $activation 
        $this.x = @(0) * $size
        $this.z = @(0) * $size
        $this.bias = @(0) * $size
        $this.output = @(0) * $size
        $this.weights = @(0) * $size
    }
}

class PowerANN {
    $layers

    PowerANN() {
        Write-Host "ANN"
    }

    initializeWeights($size) {
        for ($i = 0; $i -lt $this.layers.Count; $i++) {
            for ($j = 0; $j -lt $this.layers[$i].weights.Count; $j++) {
                $this.layers[$i].weights[$j] = $this.getRandomWeightVector($size) 
            }
            $this.layers[$i].bias = $this.getRandomWeightVector($this.layers[$i].size)
            $size = $this.layers[$i].weights.Count
        }
    }

    [Object] getRandomWeightVector($num) {
        $arr = @()
        for($i = 0; $i -lt $num; $i++) {
             $arr += (Get-Random -Minimum -0.5 -Maximum 0.5)
        }
        return $arr
    }

    [double] dotProduct($v1, $v2) {
        $dot = 0

        if (($v1.Count -eq $v2.Count) -or ($v1[0].Count -eq $v2.Count) -or ($v1.Count -eq $v2[0].Count)) {
            for ($i = 0; $i -lt $v1.Count; $i++) {
                $dot += [double]$v1[$i] * [double]$v2[$i]
            }
        } else {
            Write-Host "Column and row size aren't equal!" 
        }
        return $dot
    }

    [Object] outer($a, $b) {
        $out = @(0) * $a.Count
        $row = @(0) * $b.Count
        
        for ($i = 0; $i -lt $a.Count; $i++) {
            for ($j = 0; $j -lt $b.Count; $j++) {
                $row[$j] = $a[$i] * $b[$j]  
            }
            $out[$i] = @($row)
        }
        return $out
    }

    [double] sum($v) {
        $sum = 0

        for ($i = 0; $i -lt $v.Count; $i++) {
            $sum += $v[$i]
        }
        return $sum
    }

    [Object] transpose($v) {
        $row = @(0) * $v.Count
        $t = @(0) * $v[0].Count
        for ($i = 0; $i -lt $t.Count; $i++) {
            for ($j = 0; $j -lt $v.Count; $j++) {
                # Add columns to row
                $row[$j] = $v[$j][$i]
            }
            $t[$i] = @($row)
        }
        return $t
    }

    [Object] updateWeight($v, $s) {
        for ($i = 0; $i -lt $v.Count; $i++) { 
            $v[$i] -= $s
        }
        return $v
    }

    # Activation function #

    [double] logit($x) {
        #Write-Host "x:" $x
        return [Math]::Log($x/(1-$x))
    }

    [double] relu($x) {
        return [Math]::Max(0, [Math]::Round($x))
    }

    [double] sigmoid($x) {
        return 1/(1+[Math]::Exp(-$x))
    }

    [double] sigmoidDerivative($z) {
        return $z * (1 - $z)
    }

    [object] softmax($z) {
        $s = 0
        $v = @(0) * $z.Count

        # Compute denominator
        foreach ($input in $z) {
            $s += [Math]::Exp($input)
        }

        # Compute distribution
        for ($i = 0; $i -lt $z.Count; $i++) {
            $v[$i] = [Math]::Exp($z[$i]) / $s
        }
        return $v
    }

    [Object] sumSquaredError($y, $yh) {
        $loss = 0
        for ($i = 0; $i -lt $y.Count; $i++) {
            $loss += ($y[$i]-$yh[$i])*($y[$i]-$yh[$i])
        }
        return 0.5*$loss
    }

    [double] tanh($x) {
        return ([Math]::Exp($x) - [Math]::Exp(-$x)) / ([Math]::Exp($x) + [Math]::Exp(-$x))
    }

    addLayer($size, $activation) {
        $this.layers += @([Layer]::new($size, $activation)) 
    }

    [Object] forwardPropagation($input) {
        for ($i = 0; $i -lt $this.layers.Count; $i++) {
            $this.layers[$i].x = $input
            for ($j = 0; $j -lt $this.layers[$i].weights.Count; $j++) {
                # Linear activation
                $this.layers[$i].z[$j] = $this.dotProduct($this.layers[$i].weights[$j], $input) + $this.layers[$i].bias[$j]
                if ($this.layers[$i].activation -ne 2) {
                    # Non-linear activation
                    Switch ($this.layers[$i].activation) {
                        0 { $this.layers[$i].output[$j] = $this.logit([Math]::Min(0.9999, $this.layers[$i].z[$j])) }
                        1 { $this.layers[$i].output[$j] = $this.relu($this.layers[$i].z[$j]) }
                        3 { $this.layers[$i].output[$j] = $this.sigmoid($this.layers[$i].z[$j]) }
                    }
                }
                #Write-Host "output:" $this.layers[$i].output[$j]
            }
            if ($this.layers[$i].activation -eq 2) {
                # Softmax activation
                $this.layers[$i].output = $this.softmax($this.layers[$i].z)
            }

            $input = $this.layers[$i].output
        }
        return $this.layers[$this.layers.Count-1].output
    }

    [double] softmaxLoss($y, $num) {
        $e = 0
        if ($this.layers[$this.layers.Count - 1].activation -eq 2) {
            for ($i = 0; $i -lt $this.layers[$this.layers.Count-1].output.Count; $i++) { 
                $e -= ([Math]::Log($this.layers[$this.layers.Count-1].output[$i]) * $y[$num][$i])
            }
        }
        return $e
    }

    updateLayer($index, $dw, $learningRate) {
        for ($i = 0; $i -lt $this.layers[$index].weights.Count; $i++) {
            for ($j = 0; $j -lt $this.layers[$index].weights[$i].Count; $j++) {         
                $this.layers[$index].weights[$i][$j] -= $dw[$i][$j] * $learningRate
            }
        }
    }

    [bool] backwardPropagation($y, $num, $learningRate) {
        $s = @(0) * $this.layers[$this.layers.Count-1].weights.Count
        $dh = @(0) * $this.layers[$this.layers.Count-1].weights.Count
        $db = @(0) * $this.layers.Count
        $dw = @(0) * $this.layers.Count
        $db = 0 
        
        # Calculate error signal (softmax gradient)
        for ($i = 0; $i -lt $this.layers[$this.layers.Count-1].weights.Count; $i++) {
            if ($this.layers[$this.layers.Count - 1].activation -eq 2) {
                $s[$i] = $this.layers[$this.layers.Count-1].output[$i] - $y[$num][$i]
            } else {
                $s[$i] = -($y[$num][$i] - $this.layers[$this.layers.Count-1].output[$i]) * $this.sigmoidDerivative($this.layers[$this.layers.Count-1].output[$i])
            }
        }

        # Calculate weight derivative and output gradient
        $db = @($s)
        $dw = $this.outer($s, $this.layers[$this.layers.Count-1].x)
        $wT = $this.transpose($this.layers[$this.layers.Count-1].weights)

        $this.updateLayer($this.layers.Count-1, $dw, $learningRate)

        $outGrad = @(0) * $wT.Count
        for ($i = 0; $i -lt $wT.Count; $i++) {
            $outGrad[$i] = $this.dotProduct($wT[$i], $s) 
        }
        
        # Hidden layer Calculate weight gradients
        $errorGrad = @(0) * ($this.layers.Count-1)       
        for ($i = $this.layers.Count-2; $i -ge 0; $i--) {
            # Calculate node derivatives
            $d = @(0) * $this.layers[$i].output.Count
            for ($j = 0; $j -lt $this.layers[$i].output.Count; $j++) {
                $d[$j] = $this.sigmoidDerivative($this.layers[$i].output[$j])
            }

            # Calculate error gradient w.r.t weight
            $errorGrad[$i] = @(0) * $outGrad.Count
            for ($j = 0; $j -lt $outGrad.Count; $j++) {
                $errorGrad[$i][$j] = $d[$j] * $outGrad[$j]
            }
            
            $dw = $this.outer($errorGrad[$i], $this.layers[$i].x)
            $WT = $this.transpose($this.layers[$i].weights)
            $outGrad = @(0) * $wT.Count
            
            for ($j = 0; $j -lt $wT.Count; $j++) {
                $outGrad[$j] = $this.dotProduct($wT[$j], $errorGrad[$i]) 
            }

            $this.updateLayer($i, $dw, $learningRate)
        }
        return 0
    }

    test($data, $y) {
        $correct = 0

        for ($i = 0; $i -lt $data.Count; $i++) {
            $g = $this.forwardPropagation($data[$i])

            $index = 0
            $max = $g[0]
            for ($j = 0; $j -lt $g.Count; $j++) {
                if ($g[$j] -gt $max) {
                    $max = $g[$j]
                    $index = $j
                }
            }
            $class = $index + 1
            $real = 0

            if ($y[$i][0] -eq 1) {
                $real = 1
            } elseif ($y[$i][1] -eq 1) {
                $real = 2
            } else {
                $real = 3
            }

            if ($class -eq $real) {
                $correct++
            }

            Write-Host "guess class " $class "  real:" $real
            Write-Host "probabilities:" $this.forwardPropagation($data[$i]) "  real:" $y[$i]
            Write-Host "Accuracy: " + ($correct/($i+1))
            Write-Host " "
        }
    }
    
    train($x, $y, $signal, $epochs, $learningRate) {
        # Initialize weights
        $this.initializeWeights($x[0].Count)

        for ($k = 0; $k -lt $epochs; $k++) {
            Write-Host $k
            for ($count = 0; $count -lt $x.Count; $count++) {
                $input = $x[$count]

                $this.forwardPropagation($input)
                $this.backwardPropagation($y, $count, $learningRate)
                
                $loss = $this.softmaxLoss($y, $count)
                if ($loss -lt $signal) {
                    return
                }
            }
        }
    }

    printNodes() {
        Write-Host "Printing nodes"
        for ($i = 0; $i -lt $this.layers.Count; $i++) {
            for ($j = 0; $j -lt $this.layers[$i].weights.Count; $j++) {
                Write-Host $i $j "   b:" $this.layers[$i].bias[$j]
                Write-Host $i $j "   w:" $this.layers[$i].weights[$j]
            }
        }
    }
}
