class Layer {
    [int] $size = 0
    [int] $activation
    $z = @()
    $bias = @()
    $output = @()
    $weights = @()
        
    Layer($size, $activation) {
        $this.size = $size
        $this.activation = $activation 
        $this.z = @(0) * $size
        $this.bias = @(0) * $size
        $this.output = @(0) * $size
        $this.weights = @(0) * $size
    }
}

class PowerANN {
    $layers

    PowerANN() {
        Write-Host "ANN"
    }

    initializeWeights($size) {
        for ($i = 0; $i -lt $this.layers.Count; $i++) {
            for ($j = 0; $j -lt $this.layers[$i].weights.Count; $j++) {
                $this.layers[$i].weights[$j] = $this.getRandomWeightVector($size) 
            }
            $this.layers[$i].bias = $this.getRandomWeightVector($this.layers[$i].size)
            $size = $this.layers[$i].weights.Count
        }
    }

    [Object] getRandomWeightVector($num) {
        $arr = @()
        for($i = 0; $i -lt $num; $i++) {
             $arr += (Get-Random -Minimum -0.5 -Maximum 0.5)
        }
        return $arr
    }

    [double] dotProduct($v1, $v2) {
        $dot = 0

        for ($i = 0; $i -lt $v1.Count; $i++) {
            $dot += [double]$v1[$i] * [double]$v2[$i]
        }
        return $dot
    }

    [Object] outer($a, $b) {
        $out = @(0) * $a.Count
        $row = @(0) * $b.Count
        
        for ($i = 0; $i -lt $a.Count; $i++) {
            for ($j = 0; $j -lt $b.Count; $j++) {
                $row[$j] = $a[$i] * $b[$j]  
            }
            $out[$i] = @($row)
        }
        return $out
    }

    [double] sum($v) {
        $sum = 0

        for ($i = 0; $i -lt $v.Count; $i++) {
            $sum += $v[$i]
        }
        return $sum
    }

    [Object] updateWeight($v, $s) {
        for ($i = 0; $i -lt $v.Count; $i++) { 
            $v[$i] -= $s
        }
        return $v
    }

    # Activation function #

    [double] logit($x) {
        #Write-Host "x:" $x
        return [Math]::Log($x/(1-$x))
    }

    [double] relu($x) {
        return [Math]::Max(0, [Math]::Round($x))
    }

    [double] sigmoid($x) {
        return 1/(1+[Math]::Exp(-$x))
    }

    [double] sigmoidDerivative($z) {
        return $z * (1 - $z)
    }

    [object] softmax($z) {
        $s = 0
        $v = @(0) * $z.Count

        # Compute denominator
        foreach ($input in $z) {
            $s += [Math]::Exp($input)
        }

        # Compute distribution
        for ($i = 0; $i -lt $z.Count; $i++) {
            $v[$i] = [Math]::Exp($z[$i]) / $s
        }
        return $v
    }

    [Object] sumSquaredError($y, $yh) {
        $loss = 0
        for ($i = 0; $i -lt $y.Count; $i++) {
            $loss += ($y[$i]-$yh[$i])*($y[$i]-$yh[$i])
        }
        return 0.5*$loss
    }

    [double] tanh($x) {
        return ([Math]::Exp($x) - [Math]::Exp(-$x)) / ([Math]::Exp($x) + [Math]::Exp(-$x))
    }

    addLayer($size, $activation) {
        $this.layers += @([Layer]::new($size, $activation)) 
    }

    [Object] forwardPropagation($input) {
        for ($i = 0; $i -lt $this.layers.Count; $i++) {
            for ($j = 0; $j -lt $this.layers[$i].weights.Count; $j++) {
                if ($this.layers[$i].activation -eq 2) {
                    # Get input vector for softmax
                    $this.layers[$i].z[$j] = $this.dotProduct($this.layers[$i].weights[$j], $input) + $this.layers[$i].bias[$j]
                } else {
                    # Multiply weights by input
                    $this.layers[$i].z[$j] = $this.dotProduct($this.layers[$i].weights[$j], $input) + $this.layers[$i].bias[$j]

                    # Put through activation function
                    Switch ($this.layers[$i].activation) {
                        0 { $this.layers[$i].output[$j] = $this.logit([Math]::Min(0.9999, $this.layers[$i].z[$j])) }
                        1 { $this.layers[$i].output[$j] = $this.relu($this.layers[$i].z[$j]) }
                        3 { $this.layers[$i].output[$j] = $this.sigmoid($this.layers[$i].z[$j]) }
                    }
                }
                #Write-Host "output:" $this.layers[$i].output[$j]
            }
            if ($this.layers[$i].activation -eq 2) {
                # Put through softmax function
                $this.layers[$i].output = $this.softmax($this.layers[$i].z)
            }

            #Write-Host "output:" $this.layers[$i].output
            $input = $this.layers[$i].output
        }
        return $this.layers[$this.layers.Count-1].output
    }

    backwardPropagation($y, $num, $learningRate) {
        $e = 0
        $s = @(0) * $this.layers[$this.layers.Count-1].weights.Count
        $dh = @(0) * $this.layers[$this.layers.Count-1].weights.Count
        $db = @(0) * $this.layers.Count
        $dw = @(0) * $this.layers.Count
        
        # Calculating the error
        #$e = $this.sumSquaredError($y[$num], $this.layers[$this.layers.Count - 1].output) 

        # Calculate softmax loss
        if ($this.layers[$this.layers.Count - 1].activation -eq 2) {
            for ($i = 0; $i -lt $this.layers[$this.layers.Count-1].output.Count; $i++) { 
                $e -= ([Math]::Log($this.layers[$this.layers.Count-1].output[$i]) * $y[$num][$i])
            }
        }

        $this.printNodes()
        Write-Host "e66:  loss:" $e "  out:" $this.layers[$this.layers.Count-1].output "   real:" $y[$num]
        

        # Calculate error signal
        for ($i = 0; $i -lt $this.layers[$this.layers.Count-1].weights.Count; $i++) {
            if ($this.layers[$this.layers.Count - 1].activation -eq 2) {
                $s[$i] = $this.layers[$this.layers.Count-1].output[$i] - $y[$num][$i]
            } else {
                $s[$i] = -($y[$num][$i] - $this.layers[$this.layers.Count-1].output[$i]) * $this.sigmoidDerivative($this.layers[$this.layers.Count-1].output[$i])
            }

            # Calculate partial derivative of Error w.r.t. input
            $dh[$i] = $this.sum($this.layers[$this.layers.Count-1].weights[$i]) * $s[$i]
            #Write-Host "w:" $this.layers[$this.layers.Count-1].weights[$i] " dh:" $dh[$i]
                    
            # Calculate output weight derivative
            for ($j = 0; $j -lt $this.layers[$this.layers.Count-1].weights[$i].Count; $j++) {
                $dw = $learningRate * $s[$i] * $this.layers[$this.layers.Count-2].output[$j]
                
                #Write-Host "                                                                                                              node:" $i " d:" $d
                $this.layers[$this.layers.Count-1].weights[$i][$j] -= $dw * $learningRate
            }

            #Write-Host "                                                                                                              y:" $y[$num][$i] " out:" $this.layers[$this.layers.Count-1].output[$i] " dw: " $dw
        }

        $outer = $this.outer($s, $this.layers[$this.layers.Count-2].output)
        $outGrad = $this.dotProduct($this.layers[$this.layers.Count-1].weights, $s) 
        Write-Host "s:" $s " output:" $this.layers[$this.layers.Count-2].output
        Write-Host " "
        Write-Host "outer:" $outer
        Write-Host "outGrad:" $outGrad

        # Hidden layer Calculate weight gradients
        for ($i = $this.layers.Count-2; $i -ge 0; $i--) {

            # Store this layers weights for dh
            $tempDH = @(0) * $this.layers[$i].weights.Count
            
            $dh = $this.sum($dh)

            for ($j = 0; $j -lt $this.layers[$i].weights.Count; $j++) {
                $weights = $this.sum($this.layers[$i].weights[$j])

                # Update each weight for each error
                for ($k = 0; $k -lt $this.layers[$i].weights[$j].Count; $k++) {
                    #for ($l = 0; $l -lt $s.Count; $l++) {
                        $tempDH = $weights * $dh

                        # $s[$l] * $this.layers[$i].weights[$j][$k]
                        $dw = $learningRate * $dh * $this.sigmoidDerivative($this.layers[$i].z[$j])
                    
                        Write-Host "hidden  " $i $j "  dw:" $dw
                        $this.layers[$i].weights[$j][$k] -= $dw
                    #}
                }

                # Update bias
                #$this.layers[$i].bias[$j] += $this.sum($s)
            }
            $dh = $tempDH
        }
    }

    test($data, $y) {
        for ($i = 0; $i -lt $data.Count; $i++) {
            Write-Host "input:" $data[$i] " guess:" $this.forwardPropagation($data[$i]) " real:" $y[$i]
        }
    }
    
    train($data, $y, $loss, $epochs, $learningRate) {
        # Initialize weights
        Write-Host "Vector size" $data[0].Count
        $this.initializeWeights($data[0].Count)
        $this.printNodes()

        for ($k = 0; $k -lt $epochs; $k++) {
            Write-Host $k
            for ($count = 0; $count -lt $data.Count; $count++) {
                $input = $data[$count]

                # Forward propagation
                $this.forwardPropagation($input)

                # Backpropagation
                $this.backwardPropagation($y, $count, $learningRate) 
            }
        }
    }

    printNodes() {
        Write-Host "Printing nodes"
        for ($i = 0; $i -lt $this.layers.Count; $i++) {
            for ($j = 0; $j -lt $this.layers[$i].weights.Count; $j++) {
                Write-Host $i $j "   b:" $this.layers[$i].bias[$j]
                Write-Host $i $j "   w:" $this.layers[$i].weights[$j]
            }
        }
    }
}
<#
Import-Module $env:HOMEPATH\documents\Code\Powershell\ML\MLHelper.ps1

$n = [PowerANN]::new()
$ml = [MLHelper]::new()

$data = ,@(1)
$data += ,@(2)
$data += ,@(4)
$data += ,@(6)
$data += ,@(8)
$data += ,@(1)
$data += ,@(3)
$data += ,@(30)

$y = @(0, 1, 1, 1, 1, 0, 0, 1)

$data = $ml.normalizeData($data)
$y = $ml.normalizeData($y)

Write-Host "y:" $y  " x:" $data

# Hidden Layers
$n.addLayer(3, 0)
$n.addLayer(6, 0)
$n.addLayer(3, 0)

# Output layer
$n.addLayer(1, 0)

$n.train($data, $y, 0, 100, 0.05)
#$n.printNodes()

#Write-Host "Dot: " $n.dotProduct($data[0], $data[1])
#Write-Host "Sigmoid: " $n.sigmoid(5)

Write-Host "1, 2, 3" $n.forwardPropagation($data[0])

Write-Host "4, 8, 3" $n.forwardPropagation($data[1])

Write-Host "4, 1, 3" $n.forwardPropagation($data[4])

Write-Host "30, 25, 6" $n.forwardPropagation($data[7])
#>

$t = 3.0, 1.0, 0.2

$n = [PowerANN]::new()
Write-Host $n.outer((1, 2, 3), (4, 5, 6))
Write-Host $n.sigmoid(5)
