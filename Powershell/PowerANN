class Layer {
    [int] $size = 0
    [int] $activation
    $z = @()
    $bias = @()
    $output = @()
    $weights = @()
        
    Layer($size, $activation) {
        $this.size = $size
        $this.activation = $activation 
        $this.z = @(0) * $size
        $this.bias = @(0) * $size
        $this.output = @(0) * $size
        $this.weights = @(0) * $size
    }
}

class PowerANN {
    $layers

    PowerANN() {
        Write-Host "ANN"
    }

    initializeWeights($size) {
        for ($i = 0; $i -lt $this.layers.Count; $i++) {
            for ($j = 0; $j -lt $this.layers[$i].weights.Count; $j++) {
                $this.layers[$i].weights[$j] = $this.getRandomWeightVector($size) 
            }
            $this.layers[$i].bias = $this.getRandomWeightVector($this.layers[$i].size)
            $size = $this.layers[$i].weights.Count
        }
    }

    [Object] getRandomWeightVector($num) {
        $arr = @()
        for($i = 0; $i -lt $num; $i++) {
             $arr += (Get-Random -Minimum -1.0 -Maximum 1.0)
        }
        return $arr
    }

    [double] dotProduct($v1, $v2) {
        $dot = 0

        for ($i = 0; $i -lt $v1.Count; $i++) {
            $dot += $v1[$i] * $v2[$i]
        }
        return $dot
    }

    [Object] updateWeight($v, $s) {
        for ($i = 0; $i -lt $v.Count; $i++) { 
            $v[$i] -= $s
        }
        return $v
    }

    # Activation function #

    [double] logit($x) {
        #Write-Host "x:" $x
        return [Math]::Log($x/(1-$x))
    }

    [double] relu($x) {
        return [Math]::Max(0, [Math]::Round($x))
    }

    [double] sigmoid($x) {
        return 1/(1+[Math]::Exp(-$x))
    }

    [double] sigmoidDerivative($z) {
        return $z * (1 - $z)
    }

    [object] softmax($z) {
        $s = 0
        $v = @(0) * $z.Count

        # Compute denominator
        foreach ($input in $z) {
            $s += [Math]::Exp($input)
        }

        # Compute distribution
        for ($i = 0; $i -lt $z.Count; $i++) {
            $v[$i] = [Math]::Exp($z[$i]) / $s
        }
        return $v
    }

    [Object] sumSquaredError($y, $yh) {
        $loss = 0
        for ($i = 0; $i -lt $y.Count; $i++) {
            $loss += ($y[$i]-$yh[$i])*($y[$i]-$yh[$i])
        }
        return 0.5*$loss
    }

    [double] tanh($x) {
        return ([Math]::Exp($x) - [Math]::Exp(-$x)) / ([Math]::Exp($x) + [Math]::Exp(-$x))
    }

    addLayer($size, $activation) {
        $this.layers += @([Layer]::new($size, $activation)) 
    }

    [Object] forwardPropagation($input) {
        for ($i = 0; $i -lt $this.layers.Count; $i++) {
            for ($j = 0; $j -lt $this.layers[$i].weights.Count; $j++) {
                if ($this.layers[$i].activation -eq 2) {
                    # Get input vector for softmax
                    $this.layers[$i].z[$j] = $this.dotProduct($this.layers[$i].weights[$j], $input) + $this.layers[$i].bias[$j]
                } else {
                    # Multiply weights by input
                    $this.layers[$i].z[$j] = $this.dotProduct($this.layers[$i].weights[$j], $input) + $this.layers[$i].bias[$j]

                    # Put through activation function
                    Switch ($this.layers[$i].activation) {
                        0 { $this.layers[$i].output[$j] = $this.logit([Math]::Min(0.9999, $this.layers[$i].z[$j])) }
                        1 { $this.layers[$i].output[$j] = $this.relu($this.layers[$i].z[$j]) }
                        3 { $this.layers[$i].output[$j] = $this.sigmoid($this.layers[$i].z[$j]) }
                    }
                }
                #Write-Host "output:" $this.layers[$i].output[$j]
            }
            if ($this.layers[$i].activation -eq 2) {
                # Put through softmax function
                $this.layers[$i].output = $this.softmax($this.layers[$i].z)
            }

            #Write-Host "output:" $this.layers[$i].output
            $input = $this.layers[$i].output
        }
        return $this.layers[$this.layers.Count-1].output
    }

    backwardPropagation($y, $num, $learningRate) {
        $d = 0

        # Calculating the error
        $e = $this.sumSquaredError($y[$num], $this.layers[$this.layers.Count - 1].output) 

        # Calculate softmax gradient
        if ($this.layers[$this.layers.Count - 1].activation -eq 2) {
            for ($i = 0; $i -lt $this.layers[$this.layers.Count-1].output.Count; $i++) { 
                $d += $this.layers[$this.layers.Count - 1].z[$i] * ($y[$num][$i] - $this.layers[$this.layers.Count-1].output[$i])
            }
        }

        Write-Host "e34:" $e " loss:" $d "  out:" $this.layers[$this.layers.Count-1].output "   real:" $y[$num]

        # Update output layer
        for ($i = 0; $i -lt $this.layers[$this.layers.Count-1].weights.Count; $i++) {
            # Find gradient with respect to weight: (out - target)*out(1 - out)*(outh * w) 
            if ($this.layers[$this.layers.Count - 1].activation -ne 2) {
                #$d = -($y[$num][$i] - $this.layers[$this.layers.Count-1].output[$i]) * $this.sigmoidDerivative($this.layers[$this.layers.Count-1].output[$i])
            } else {
                $d = -($y[$num][$i] - $this.layers[$this.layers.Count-1].output[$i]) * $this.sigmoidDerivative($this.layers[$this.layers.Count-1].output[$i])
            }

            Write-Host "                                                                                                              y:" $y[$num][$i] " O:" $this.layers[$this.layers.Count-1].output[$i] " d: " $d

            for ($j = 0; $j -lt $this.layers[$this.layers.Count-1].weights[$i].Count; $j++) {
                # Calculate derivative
                $dw = $learningRate * $d * $this.layers[$this.layers.Count-2].output[$j]
                
                Write-Host "                                                                                                              dw:" $dw
                $this.layers[$this.layers.Count-1].weights[$i][$j] -= $dw
            }
        }

        # Update hidden layers
        for ($i = 0; $i -lt $this.layers.Count-1; $i++) {
            for ($j = 0; $j -lt $this.layers[$i].weights.Count; $j++) {
                #$d = $this.sigmoidDerivative($this.layers[$i].output[$j])
                for ($k = 0; $k -lt $this.layers[$i].weights[$j].Count; $k++) {
                    $d = $d * -$this.layers[$i].weights[$j][$k]

                    #$dw = $this.dotProduct($this.layers[$i].output[0]/$y[0].Count, $loss)
                    
                    #Write-Host $this.layers[$i].weights[$j]
                    $dw = $learningRate * $d * $this.layers[$i].z[$j]
                    
                    #Write-Host "hidden  " $i $j $e "  dw:" $dw
                    #$this.layers[$i].weights[$j][$k] -= $dw
                }
            }
        }
    }

    test($data, $y) {
        for ($i = 0; $i -lt $data.Count; $i++) {
            Write-Host "input:" $data[$i] " guess:" $this.forwardPropagation($data[$i]) " real:" $y[$i]
        }
    }
    
    train($data, $y, $loss, $epochs, $learningRate) {
        # Initialize weights
        Write-Host "Vector size" $data[0].Count
        $this.initializeWeights($data[0].Count)
        $this.printNodes()

        for ($k = 0; $k -lt $epochs; $k++) {
            #Write-Host $k
            for ($count = 0; $count -lt $data.Count; $count++) {
                $input = $data[$count]

                # Forward propagation
                $this.forwardPropagation($input)

                # Backpropagation
                $this.backwardPropagation($y, $count, $learningRate) 
            }
        }
    }

    printNodes() {
        Write-Host "Printing nodes"
        for ($i = 0; $i -lt $this.layers.Count; $i++) {
            for ($j = 0; $j -lt $this.layers[$i].weights.Count; $j++) {
                Write-Host $i $j "   b:" $this.layers[$i].bias[$j]
                Write-Host $i $j "   w:" $this.layers[$i].weights[$j]
            }
        }
    }
}
